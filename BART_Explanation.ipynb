{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BART Explanation",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4tbK8+i0YKskwA0FXw6Y8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinLolochum/BERT-MODELS/blob/main/BART_Explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqSY7kEG3G-6"
      },
      "source": [
        "**BART**- Bidectional Auto_Regressive Transformers "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyjyyJCT_Zpc"
      },
      "source": [
        "**BART MODEL**\r\n",
        "\r\n",
        "- **B**idiresctional **A**uto_**R**egressive **T**ransformers. Is a denoising autoencoder that maps a corrupted document to the original document it was derived from.\r\n",
        "- It is implemented as a sequence-to-sequence model\r\n",
        "with a bidirectional encoder over corrupted text and a\r\n",
        "left-to-right autoregressive decoder. \r\n",
        "- For pre-training, it optimizes the negative log likelihood of the original\r\n",
        "document.\r\n",
        "\r\n",
        "**Architecture**\r\n",
        "- BART uses the standard sequence-to-sequence Transformer\r\n",
        "architecture from (Vaswani et al., 2017), \r\n",
        "- Followed by GPT, with modified ReLU activation\r\n",
        "functions to GeLUs (Hendrycks & Gimpel, 2016)\r\n",
        "- and initialise parameters from N(0; 0:02). \r\n",
        "- For the base model, it uses 6 layers in the encoder and decoder 12 layers for the large model.\r\n",
        "\r\n",
        "*similar to BERT architecture except*\r\n",
        "\r\n",
        "- (1) each layer of the decoder additionally performs cross-attention over\r\n",
        "the final hidden layer of the encoder (as in the transformer\r\n",
        "sequence-to-sequence model); and \r\n",
        "- (2) BERT uses an additional feed-forward network before wordprediction,\r\n",
        "which BART does not. \r\n",
        "- In total, BART contains roughly 10% more parameters than the equivalently\r\n",
        "sized BERT model.\r\n",
        "\r\n",
        "**Pretraining**\r\n",
        "- Pre-trained by corrupting documents then optimizing the reconstruction error- ( the cross enthropy between the decoder outputs and the original document).\r\n",
        "- Types of noising transformations applied include. **Token masking**, **TOken deletion**, **text infilling**, **Sentence permutation**, **Document rotation**\r\n",
        "\r\n",
        "**Fine-tuning**\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "- Pre-training is in two stages. (1) Data is corrupted by adding an arbitrary noising function. (2) The sequence to sequenc model is learned to reconstruct the original text.\r\n",
        "- Very effective in text generation but also achieves SOTA results in abstractive, question answering and summarization tasks.\r\n",
        "- Opens new ways of thinking about fine_tuning.\r\n",
        "- Offers new skim for machine translation by stacking BART layers on top of transformer layers.\r\n",
        "\r\n",
        "**Good AT**\r\n",
        "- Text generation, NLU, summarization, sentiment analysis\r\n",
        "\r\n",
        "**PROS**\r\n",
        "- It is an improvement of the previous models, BERT, RoberTA, GPT-2. Therefore it achives higher SOTA results in several tasks compared to the above mentioned models.\r\n",
        "\r\n",
        "**CONS**\r\n",
        "- Requires more trianing resources compared to BERT and earlier models. \r\n"
      ]
    }
  ]
}