{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT- Question Answering",
      "provenance": [],
      "authorship_tag": "ABX9TyOYH25h6tDsp4zVDmgtAhlF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinLolochum/BERT-FORMS/blob/main/BERT_Question_Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anfn92YJqK6s"
      },
      "source": [
        "This is the first question answering text, I will add most of the explanations about the model and the parameters here and less in future similar tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPoexZG8L9w8"
      },
      "source": [
        "Install transformers Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD2QijQuMC2l"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phCxmwEPMQlS"
      },
      "source": [
        "Import important libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcbluAigMPqq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import tensorflow_datasets as tfds\n",
        "from transformers import BertForQuestionAnswering, BertTokenizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODojJHSsWbhr"
      },
      "source": [
        "Import the **S**tanford **Qu**estion**A**nswering **D**ataset (**SQuAD**)\n",
        "\n",
        "SQuaD 1.1 contains over 100,000 question-answer pairs on 500+ articles. In SQuAD dataset, a single sample consists of a paragraph and a set questions. The goal is to find, for each question, a span of text in a paragraph that answers that question. Model performance is measured as the percentage of predictions that closely match any of the ground-truth answers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp6DjyKoWauO"
      },
      "source": [
        "Data = tfds.load('squad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGn5YmBHYg5x"
      },
      "source": [
        "Train, Valid = Data['train'], Data['validation']\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EefQu1lmewau"
      },
      "source": [
        "Exploring and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF49EJ2QeFBE"
      },
      "source": [
        "for entry in Train.take(1):  \n",
        "  answer, question = Train[\"answers\"], example[\"question\"]\n",
        "  print(answer)\n",
        "  print(question)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrTUpxGQNj0M"
      },
      "source": [
        "Initializing model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgoeEtmaNiW7"
      },
      "source": [
        "MODEL = 'bert-base-uncased'\n",
        "\n",
        "Tokenizer = BertTokenizer.from_pretrained(MODEL)\n",
        "Model = BertForQuestionAnswering.from_pretrained(MODEL, return_dict = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCXBwyDxl7_V"
      },
      "source": [
        "Inputs/parameters. Here are the [explanations](https://huggingface.co/transformers/glossary.html#attention-mask) of what these paramenters represent.\n",
        "\n",
        "*  input_ids - Ids of word embeddings\n",
        "*  attention_masks - Values to point inputs that should be attended to, i.e inputs that are not paddings.\n",
        "*  input_type_ids - Classification and separation tokens.\n",
        "*  segment_ids - Whether the segment is a question or an answer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5P79ByGnwQg"
      },
      "source": [
        "# Returning paramenters "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_tUgOwfnvrW"
      },
      "source": [
        "Training and evaluation using Hugging face trainer. [Trainer](https://huggingface.co/transformers/training.html#trainer),  [Source Code](https://huggingface.co/transformers/_modules/transformers/trainer.html#Trainer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HrLIVnFuQF9"
      },
      "source": [
        "# Using hugging face trainer.\n",
        "# trainer.train() to train and trainer.evaluate() to evaluate.\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total # of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset            # evaluation dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}