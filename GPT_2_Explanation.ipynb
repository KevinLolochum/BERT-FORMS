{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 Explanation",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMfon8XDs6b5L2Jztqbc8nW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinLolochum/BERT-MODELS/blob/main/GPT_2_Explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BvDup-whNaD"
      },
      "source": [
        "**GPT-2 (Generative Pre-Trained transformers 2)**\r\n",
        "-  **GPT-2** paper can be found in [Language models are unsupervised multitask learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\r\n",
        "\r\n",
        "**Architecture**\r\n",
        "\r\n",
        "- GPT-2 like all other modern large NLP models uses the transformers architecture.\r\n",
        "- GPT-2 uses only the decoder layer of a transfomers architecture.\r\n",
        "- The base model has 12 decoder blocks, 768 Hidden layers and 1024 tokens and 117 Million parameters.\r\n",
        "- This increases by 12 to medium (24), large(36) and XL(48) which has 1.5 M parameters.\r\n",
        "- GPT-2's **decoder** blocks are different from BERT's **encoder** because they have masked self-attention.\r\n",
        "- This ensures that the model only uses past and present tokens for prediction unlike BERT that sees future tokens.\r\n",
        "- Because of this GPT-2 is unidirectional and moves sequentially from left to right.\r\n",
        "- The objective is simple, predict the next word using all the sequences upto the current word-token.\r\n",
        "\r\n",
        "\r\n",
        "**pretraining**\r\n",
        "- GPT-2 uses zero-shot setting for all its tasks and achieves SOTA in 7 out of 8 language tasks it is tested on. \r\n",
        "- Zero-shot training means that GPT-2 is not fine-tuned for any specific task during training and prediction.\r\n",
        "- The reseaschers who propose GPT-2 believe that the superior perfomance of GPT-2 is due in part to the large amount of data that is used in GPT-2 training (40GB of webtext data or 8 milllion webpages).\r\n",
        "- It is important to note that because of the size of the datasets used in GPT-2 there is some slight overlap in training and validation samples\r\n",
        "- This leads to a slight improvement in scores but GPT-2 still offers SOTA after this is controlled for.\r\n",
        "\r\n",
        "**Good at**\r\n",
        "- GPT is very good at predict the next sentence in a a sequence hence good at text generation.\r\n",
        "- Does well in common sense tasks like the winograd challenge, achieves SOTA.\r\n",
        "- Does very well in named entity recognition tasks, achieves SOTA Childrens Book Test (CBT).\r\n",
        "\r\n",
        "**Bad at**\r\n",
        "\r\n",
        "- It does not do well in summarization tasks.\r\n",
        "- Does not do well in question answering, beats simple models but not transformer models.\r\n",
        "\r\n",
        "\r\n",
        "**Pros**\r\n",
        "- GPT-2 is very good at some downstream tasks such as next sentence prediction because it is specifically trained for this task.\r\n",
        "- For a model that uses zero-shot setting GPT-2 achieves great even SOTA results is some tasks that it wasn't trained for.\r\n",
        "- The distilled version of GPT-2 achieves uses less compute power\r\n",
        "\r\n",
        "**Cons**\r\n",
        "- It uses a lot of data for training and is therefore computationally expensive.\r\n",
        "- GPT-2 is not fine-tuned for any tasks therefore it achieves very low results like question answering and sentiment analysis.\r\n",
        "- Because GPT-2 is trained on a lot of data (~40GB, 8 million webpages) it is hard to know how much of the results to attribute to architectural improvements vs the amount of data (memorization)"
      ]
    }
  ]
}