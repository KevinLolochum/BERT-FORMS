{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanilla BERT",
      "provenance": [],
      "authorship_tag": "ABX9TyOXnWXDiiqGGIO+/t/LljdU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinLolochum/BERT-MODELS/blob/main/Vanilla_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLrutDt4J6Cs"
      },
      "source": [
        "**Questions**\n",
        "\n",
        "-\tTheory\n",
        "-\tUSP(why it is special)\n",
        "-\tPros and cons\n",
        "-\tUsage (Code)\n",
        "-\tTraining time and inference time\n",
        "-\tLayman explanation and technical explanation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULhT7DaqrJ_A"
      },
      "source": [
        "**Theory**\n",
        "* [Paper](https://arxiv.org/abs/1810.04805)\n",
        "*   **BERT** is an improvement of transformers that allows for bidirectional training of Transformer. \n",
        "*   This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training, notably RNNs.\n",
        "*   Bert was pre-trained on two tasks, next sentence prediction (NSP) and masked language modelling (MLM) but can produce State-of-the-art (SOTA) results in other NLP tasks with fine tuning.\n",
        "*   Masking in MLM helps prevent information leakage (each layer indirectly sees itself in a multilayer bidectional traiining). In BERT 15% of the words in each sentence are [masked] to be predicted.\n",
        "*   NSP handles relationships between sentences. Which is very important for many downstream taks. The labels in NSP indicate whether sentence isnext or isnotnext. In fine-tuning it is fed the correct labels is next 50% of the time.\n",
        "*   The paper introduced two models \n",
        "*   BERT base – 12 layers (transformer blocks encoder blocks), 768 hidden size, 12 attention heads, and 110 million parameters.\n",
        "*   BERT Large – 24 layers, 1024 hidden size, 16 attention heads and, 340 million parameters.\n",
        "*   Inputs are in the form of word embeddings like any other NLP models but with two special tokens like the transformers\n",
        "*   [CLS] a classification token is prepended and [SEP] a sentence separation token is appened to each sentence.\n",
        "*  The output of BERT is at each position is a vector of size hidden_size (768 in BERT Base) \n",
        "*  You can pass this output to any classification layer, such as LSTM and it will return a prediction.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzpiX2HcqH3Q"
      },
      "source": [
        "**Pros**\n",
        "* Better prediction than unidirectional models because it keeps context.\n",
        "* Bert is pretrained therefore it uses much less compute power compared to from scratch models.\n",
        "\n",
        "**Cons**\n",
        "* Uses a lot of compute power when training because it only predicts 15% of inputs for each batch..\n",
        "*  Assigns different vectors to same words in different contexts, leading to a lot trainable parameters and compute power.\n",
        "\n",
        "**Good at**\n",
        "*  Natural language Inference\n",
        "*  Sentiment analysis\n",
        "*  Named entity recognition\n",
        "*  Question answering\n",
        "\n",
        "\n",
        "**Training and inference time**\n",
        "* Slower than distill BERT\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}