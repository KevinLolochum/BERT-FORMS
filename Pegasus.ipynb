{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pegasus",
      "provenance": [],
      "authorship_tag": "ABX9TyMGiOg/4jPmPzvomLOqwn/T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinLolochum/Transformer-style-fine-tuned-models/blob/main/Pegasus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CJBimJV8O3u"
      },
      "source": [
        "**PEGASUS**\r\n",
        "\r\n",
        "- **P**re-traing with **E**xtracted **G**ap-sentence for **A**bstractive **Su**mmarition **S**equence_to_sequence models [paper](https://arxiv.org/pdf/1912.08777.pdf)\r\n",
        "- Pegasus training is similar to summarization pretraining.\r\n",
        "- Important sentences are remove or masked from the input doc then they are generated from the remaining sentences (**G**ap **S**entence **G**eneration) like in extractive summarization.\r\n",
        "- Model also uses **MLM** on the sentences that are not masked.\r\n",
        "- The Model is self-supervised and has 568 parameters.\r\n",
        "- Achieves SOTA on all 12 downstream tasks as measured by rouge and human eval\r\n",
        "- Authors code is found [here](https://github.com/google-research/pegasus)\r\n",
        "- Distilled checkpoints found [here](https://arxiv.org/pdf/2010.13002.pdf)\r\n",
        "\r\n",
        "\r\n",
        "**Architecture**\r\n",
        "-PEGASUSBASE had L = 12;H = 768; F = 3072;A = 12\r\n",
        "- PEGASUSLARGE had L = 16;H = 1024; F =\r\n",
        "4096;A = 16, where L\r\n",
        "- Model is made up 16 encoder-decoder layers and inherits from BertForConditional Generation.\r\n",
        "- Same configuration except; uses static, sinusidual positional embeddings, model starts generating with pad_token_id and more beams are used.\r\n",
        "- All pretrained pegasus checkpoints are the same besides three attributes: tokenizer.model_max_length (maximum input size), max_length (the maximum number of tokens to generate) and length_penalty.\r\n",
        "\r\n",
        "**Pre-training**\r\n",
        "We propose a new pre-training objective, GSG, in this\r\n",
        "work, but for comparison, we also evaluate BERTâ€™s maskedlanguage\r\n",
        "model objective, in isolation and in conjunction\r\n",
        "with GSG.\r\n",
        "- **Gap sentence ratio** GSR is the ratio of masked sentences to the total number of sentences in the document. Similar to mask ratio.\r\n",
        "- Final model uses the most important sentence (pricipal sentence) but other options explored are random and lead (beginning) sentences.\r\n",
        "- 30% of the sentences are masked.\r\n",
        "- GSR is a significant parameter with low leading to less challenging and computationally efficient and high making it lose contextual information needed to guide generation.\r\n",
        "- Important sentences are determined using ROUGE_1-F_1 (lin, 2014).\r\n",
        "- MLM is not included in the final model, PEGASUS_large because it was found to not improve results under large training steps.\r\n",
        "- Pegasus uses Sentence_Pair Unigram (Unigram) for word encodings. With 96K unigram vocabulary achieving the best results."
      ]
    }
  ]
}