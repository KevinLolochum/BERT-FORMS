{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistilBERT",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8GF92Y8tZ913iN02uel9b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinLolochum/BERT-MODELS/blob/main/DistilBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLrutDt4J6Cs"
      },
      "source": [
        "**Questions**\n",
        "\n",
        "-\tTheory\n",
        "-\tUSP(why it is special)\n",
        "-\tPros and cons\n",
        "-\tUsage (Code)\n",
        "-\tTraining time and inference time\n",
        "-\tCode\n",
        "-\tLayman explanation and technical explanation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULhT7DaqrJ_A"
      },
      "source": [
        "**Theory**\n",
        "\n",
        "\n",
        "* Model architecture: 6-layer, 768-hidden, 12-heads, 65M parameters\n",
        "The DistilBERT model distilled from the BERT model bert-base-cased.\n",
        "* DistilBERT is a small, fast, cheap and light Transformer model based on Bert architecture.\n",
        "* It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving 97% of BERT's performances as measured on the GLUE language understanding benchmark.\n",
        "* DistilBERT is trained using knowledge distillation, a technique to compress a large model called the teacher into a smaller model called the student.\n",
        "* DistilBERT is trained using knowledge distillation, a technique to compress a large model called the teacher into a smaller model called the student.\n",
        "* \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzpiX2HcqH3Q"
      },
      "source": [
        "**Pros**\n",
        "* Better prediction than unidirectional models because it keeps context.\n",
        "\n",
        "**Cons**\n",
        "* Uses a lot of compute power when training because it only predicts 15% of inputs for each batch..\n",
        "*  Assigns different vectors to same words in different contexts, leading to a lot trainable parameters and compute power.\n",
        "\n",
        "**Good at**\n",
        "*  Next Sentence Prediction - A task it was trained on\n",
        "*  Sentiment analysis\n",
        "*  Named entity recognition\n",
        "*  Question answering\n",
        "\n",
        "\n",
        "**Training and inference time**\n",
        "* Decreased training time compared to BERT\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}