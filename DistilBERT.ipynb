{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistilBERT",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNWK2zdMdE3Fgz1pQ2Os7m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinLolochum/BERT-MODELS/blob/main/DistilBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLrutDt4J6Cs"
      },
      "source": [
        "**Questions**\n",
        "\n",
        "-\tTheory\n",
        "-\tUSP(why it is special)\n",
        "-\tPros and cons\n",
        "-\tUsage (Code)\n",
        "-\tTraining time and inference time\n",
        "-\tLayman explanation and technical explanation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULhT7DaqrJ_A"
      },
      "source": [
        "**Theory**\n",
        "* [Paper](https://arxiv.org/abs/1910.01108)\n",
        "* Model architecture: 6-layer, 768-hidden, 12-heads, 65M parameters\n",
        "The DistilBERT model distilled from the BERT model bert-base-cased.\n",
        "* DistilBERT is a small, fast, cheap and light Transformer model based on Bert architecture.\n",
        "* Model acheives this by leveraging inductive biases learned by larger model through combining language modelling, distillation and cosine distance losses.\n",
        "* It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving 97% of BERT's performances as measured on the GLUE language understanding benchmark.\n",
        "* DistilBERT is trained using knowledge distillation, a technique to compress a large model called the teacher into a smaller model called the student.\n",
        "* DistilBERT is trained using knowledge distillation, a technique to compress a large model called the teacher into a smaller model called the student.\n",
        "* \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzpiX2HcqH3Q"
      },
      "source": [
        "**Pros**\n",
        "* Better results than some earlier NLP models such as ELMO.\n",
        "* Attains near BERT accuracy with much less comppute power and training time requirements\n",
        "\n",
        "**Cons**\n",
        "* Less accurate compared too BERT.\n",
        "\n",
        "**Good at**\n",
        "*  NLI\n",
        "*  Sentiment analysis\n",
        "*  Named entity recognition\n",
        "*  Question answering\n",
        "\n",
        "\n",
        "**Training and inference time**\n",
        "* Decreased training time compared to BERT.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq9aiClnecZE"
      },
      "source": [
        "* ***Knowledge distillation***-compression technique in which\n",
        "a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher -\n",
        "or an ensemble of models.\n",
        "\n",
        "* ***Training loss*** - DistillBert's training loss is called distillation loss - \n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "* where ti (resp. si) is a probability estimated by the teacher\n",
        "(resp. the student).\n",
        "* DistillBERT uses three losses. Distillation loss, MLM loss (cross entropy), cosine distance loss. MLM being the least important of the three."
      ]
    }
  ]
}