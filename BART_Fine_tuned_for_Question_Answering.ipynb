{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BART- Fine_tuned for Question Answering",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNzeO5bn5hUbKxjkCcuEybn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinLolochum/BERT-MODELS/blob/main/BART_Fine_tuned_for_Question_Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_YTYbFn0FRL"
      },
      "source": [
        "**Fine_tuning BART Extracitve Question Answering in PyTorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anfn92YJqK6s"
      },
      "source": [
        "- In this exampe I am assuming you have some familiarity with transformers and the Huggingface example of using an already fine-tuned model like BERT or BART for question answering.\r\n",
        "- If you don't, you can read from [here](https://huggingface.co/transformers/model_doc/bert.html#bertforquestionanswering) or most online articles about BERT for quesion answering.\r\n",
        "- What I am trying to answer is the how. How hugging face fine_tuned this model I have done examples on BERT, DistilBERT and therefore the code will be very similar to the one I allreaddy used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD2QijQuMC2l"
      },
      "source": [
        "# Install transformers and huggingface datasets\r\n",
        "!pip install transformers\r\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phCxmwEPMQlS"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcbluAigMPqq"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from transformers import BartForQuestionAnswering, BartTokenizerFast, get_linear_schedule_with_warmup\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6FDw-Oghu3u"
      },
      "source": [
        "**1. Instantiate model**\r\n",
        "\r\n",
        "- I will be inheriting from the bert_base_uncased and the BertQuestion answering framework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNWt3nV-I4yP"
      },
      "source": [
        "# We are using BertTokenizerFast because other python tokens do not have char_to_token functionality we will need later.\r\n",
        "\r\n",
        "tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-base')\r\n",
        "model = BartForQuestionAnswering.from_pretrained('facebook/bart-base', return_dict = True)\r\n",
        "\r\n",
        "optimizer = Adam(model.parameters(), lr=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZPsY-QDipJJ"
      },
      "source": [
        "**2. Data**\r\n",
        "\r\n",
        "- I will be using **S**tanford **Qu**estion**A**nswering **D**ataset (**SQuAD**)\r\n",
        "- SQuAD is a pre_cleaned question answering dataset but I will apply a few changes to get correct answer alignments\r\n",
        "\r\n",
        "- You can explore the dataset [here](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/), download on tfds, huggingface datasets or Kaggle.\r\n",
        "* The goal is to find, for each question, a span of text in a paragraph that answers that question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwoGxKpGJVpe"
      },
      "source": [
        "# Loading squad dataset from \r\n",
        "from datasets import load_dataset\r\n",
        "\r\n",
        "# Load and split dataset, using small datasets for the sake of model training\r\n",
        "train_data, valid_data = load_dataset('squad', split='train[:1%]'), load_dataset('squad', split='validation[:1%]')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41x089YYKJg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c6ae7a-1ab6-4637-b54d-300b62346da3"
      },
      "source": [
        "# Checking the features of the answers \r\n",
        "train_data.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(876, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSiNoRZDkdv0"
      },
      "source": [
        "- Getting correct answer text alignment and tokenizing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Pmox87IuwU"
      },
      "source": [
        "# Dataset cleaning and tokenization\r\n",
        "# BertTokenizerFast because python tokenizer do not have char_to_token functionality\r\n",
        "\r\n",
        "def correct_alignment(context, answer):\r\n",
        "\r\n",
        "    \"\"\" Description: This functions corrects the alignment of answers in the squad dataset that are sometimes off by one or 2 values also adds end_postion index.\r\n",
        "    \r\n",
        "    inputs: list of contexts and answers\r\n",
        "    outputs: Updated list that contains answer_end positions \"\"\"\r\n",
        "    \r\n",
        "    start_text = answer['text'][0]\r\n",
        "    start_idx = answer['answer_start'][0]\r\n",
        "    end_idx = start_idx + len(start_text)\r\n",
        "\r\n",
        "    # When alignment is okay\r\n",
        "    if context[start_idx:end_idx] == start_text:\r\n",
        "      return start_idx, end_idx    \r\n",
        "      # When alignment is off by 1 character\r\n",
        "    elif context[start_idx-1:end_idx-1] == start_text:\r\n",
        "      return start_idx-1, end_idx-1  \r\n",
        "      # when alignment is off by 2 characters\r\n",
        "    elif context[start_idx-2:end_idx-2] == start_text:\r\n",
        "      return start_idx-2, end_idx-2\r\n",
        "    else:\r\n",
        "      raise ValueError()\r\n",
        "\r\n",
        "\r\n",
        "# Tokenize our training dataset\r\n",
        "def convert_to_features(example_batch):\r\n",
        "  \"\"\" Description: This functions tokenizes the context and questions then appends encoded start_positions and end_positions from the above function.\r\n",
        "    \r\n",
        "    inputs: list of contexts, questions and answers\r\n",
        "    outputs: Updated list that contains answer_end positions \"\"\"\r\n",
        "\r\n",
        "    # Tokenize contexts and questions (as pairs of inputs)\r\n",
        "  encodings = tokenizer(example_batch['context'], example_batch['question'], truncation=True)\r\n",
        "\r\n",
        "    # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methods.\r\n",
        "  start_positions, end_positions = [], []\r\n",
        "  for i, (context, answer) in enumerate(zip(example_batch['context'], example_batch['answers'])):\r\n",
        "    start_idx, end_idx = correct_alignment(context, answer)\r\n",
        "    start_positions.append(encodings.char_to_token(i, start_idx))\r\n",
        "    end_positions.append(encodings.char_to_token(i, end_idx-1))\r\n",
        "    # update encodings   \r\n",
        "  encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\r\n",
        "\r\n",
        "  return encodings"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VMq6CAoLMqd"
      },
      "source": [
        "# Map the dataset to the convert_function, faster than using for loops.\r\n",
        "\r\n",
        "Training_encoded = train_data.map(convert_to_features, batched=True)\r\n",
        "Validation_encoded = valid_data.map(convert_to_features, batched = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG527bmgrLvn"
      },
      "source": [
        "# Our encoded dataset has some columns we don't need\r\n",
        "Training_encoded.features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdeFLleLVFN"
      },
      "source": [
        "# Format our encided datasets to outputs torch.Tensor to train our pytorch model\r\n",
        "\r\n",
        "columns = ['input_ids', 'attention_mask', 'start_positions', 'end_positions']\r\n",
        "Training_encoded.set_format(type='torch', columns=columns)\r\n",
        "Validation_encoded.set_format(type='torch', columns=columns)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtuHNzmYtNBf"
      },
      "source": [
        "column_names =['answers', 'context', 'id', 'question', 'title']\r\n",
        "\r\n",
        "Validation_encoded.remove_columns_(column_names=column_names)\r\n",
        "Training_encoded.remove_columns_(column_names=column_names)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "452XcUObmmJn"
      },
      "source": [
        "- Loading the tensor data into dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21FbPKhBLWQv"
      },
      "source": [
        "from tqdm.notebook import tqdm\r\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "# Instantiate a PyTorch Dataloader around our dataset\r\n",
        "# Let's do dynamic batching (pad on the fly with our own collate_fn)\r\n",
        "def collate_fn(examples):\r\n",
        "    return tokenizer.pad(examples, return_tensors='pt')\r\n",
        "dataloader_val = DataLoader(Validation_encoded, collate_fn=collate_fn, batch_size= 20, sampler=SequentialSampler(Validation_encoded))\r\n",
        "dataloader = DataLoader(Training_encoded, collate_fn=collate_fn, batch_size =20, sampler= RandomSampler(Training_encoded))\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaDgHgr9MnI4"
      },
      "source": [
        "# Setting the seed for generating random numbers\r\n",
        "import random\r\n",
        "\r\n",
        "seed_val = 42\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aVg0F6unYWr"
      },
      "source": [
        "**3. Training and evaluating the model**\r\n",
        "\r\n",
        "**Inputs/parameters**. Here are the [explanations](https://huggingface.co/transformers/glossary.html#attention-mask) of what these paramenters represent.\r\n",
        "\r\n",
        "*  input_ids - Ids of word embeddings\r\n",
        "*  attention_masks - Values to point inputs that should be attended to, i.e inputs that are not paddings.\r\n",
        "*  input_type_ids - Classification and separation tokens.\r\n",
        "*  segment_ids - Whether the segment is a question or an answer.\r\n",
        "- start_positions and end_positions - Tokens representing the start and end of an answer.\r\n",
        "\r\n",
        "**outputs**\r\n",
        "* Start_logits - probabilities that the start value is an input_id x. (torch.FloatTensor of shape (batch_size, sequence_length)) – Span-start scores (before SoftMax)\r\n",
        "* End_logits - Probabilities that the end value is an input_id x. (torch.FloatTensor of shape (batch_size, sequence_length)) – Span-start scores (before SoftMax)\r\n",
        "* Other return values are loss (cross enhropy loss). Hidden states and attention heads when specified.\r\n",
        "- Start_Loss is calculated by comparing the correct start_posistions with the start_logits from the QuestionAnswering class. \r\n",
        "- Then  end_Loss is calculated by comparing the correct end_posistions with the end_logits from the QuestionAnswering class.\r\n",
        "- The two losses are added then devided by two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-NcJnRZA5DC"
      },
      "source": [
        "# Validation function for the model\r\n",
        "\r\n",
        "def model_validation(dataloader_val):\r\n",
        "\r\n",
        "    model.eval().to(device)\r\n",
        "    \r\n",
        "    for batch in dataloader_val:      \r\n",
        "        batch = tuple(b.to(device) for b in batch)\r\n",
        "\r\n",
        "        with torch.no_grad():        \r\n",
        "            outputs = model(**batch)\r\n",
        "            \r\n",
        "        loss = outputs.loss\r\n",
        "        val_total_loss += loss.item()\r\n",
        "    return val_total_loss\r\n",
        "\r\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yTuev0ZA4Ko"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60suROPR1FDL"
      },
      "source": [
        "# Import scheduler and omptimizer \r\n",
        "from tqdm.notebook import tqdm\r\n",
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "\r\n",
        "#Clear cache before running model\r\n",
        "torch.cuda.empty_cache()\r\n",
        "\r\n",
        "epochs = 10\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \r\n",
        "                                            num_warmup_steps=0,\r\n",
        "                                            num_training_steps=len(dataloader_val)*epochs) \r\n",
        "\r\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "\r\n",
        "for epoch in tqdm(range(1, epochs+1)):\r\n",
        "    \r\n",
        "    model.train().to(device)\r\n",
        "    \r\n",
        "    loss_train_total = 0\r\n",
        "\r\n",
        "    progress_bar = tqdm(dataloader, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\r\n",
        "    for batch in progress_bar:\r\n",
        "\r\n",
        "        model.zero_grad()\r\n",
        "        \r\n",
        "        batch.to(device)      \r\n",
        "\r\n",
        "        outputs = model(**batch)\r\n",
        "        \r\n",
        "        loss = outputs.loss\r\n",
        "        loss_train_total += loss.item()\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "        optimizer.step()\r\n",
        "        scheduler.step()\r\n",
        "        \r\n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\r\n",
        "         \r\n",
        "        \r\n",
        "    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\r\n",
        "        \r\n",
        "    tqdm.write(f'\\nEpoch {epoch}')\r\n",
        "    \r\n",
        "    loss_train_avg = loss_train_total/len(dataloader)            \r\n",
        "    tqdm.write(f'Training loss: {round(loss_train_avg, 2)}')\r\n",
        "    \r\n",
        "    val_loss = model_validation(dataloader_val)\r\n",
        "    tqdm.write(f'Validation loss: {round(val_loss, 2)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azmfqp6ss6Od"
      },
      "source": [
        "- Model not run because of GPU constraints but if you have enough GPU/CPU space it works.\r\n",
        "- You can train, save and load for predictions.\r\n"
      ]
    }
  ]
}